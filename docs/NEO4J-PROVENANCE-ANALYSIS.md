# Neo4j Provenance-First Architecture - Analysis & Adaptation

**Date**: October 12, 2025  
**Status**: Strategic Architecture Decision  
**Impact**: HIGH - Affects entire ingestion pipeline and data model

---

## ðŸŽ¯ Executive Summary

**Recommendation**: Implement Neo4j knowledge graph architecture **BEFORE** FastAPI real-time streaming.

**Why**: This is a **foundational architecture change** that transforms how we ingest, store, and query content. The FastAPI streaming should stream progress for the NEW pipeline, not the old one. Doing FastAPI first would mean rebuilding it immediately after.

**Approach**: Hybrid Neo4j + Qdrant strategy, phased implementation over 3 sprints.

---

## ðŸ“Š Current vs. Proposed Architecture

### Current State (Sprints 1-4)

```
URL â†’ Extract â†’ Transform â†’ Load
                    â†“
              Basic metadata
                    â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â†“                     â†“
    Qdrant vectors      MinIO files
    (embeddings)        (markdown)
         â†“                     â†“
    Vector search       File storage
```

**Capabilities**:
- âœ… Basic extraction (HTML, PDF, YouTube)
- âœ… Simple transformation (summary, tags)
- âœ… Vector embeddings (Qdrant)
- âœ… File storage (MinIO)
- âœ… Simple search

**Missing**:
- âŒ No entity recognition (NER)
- âŒ No entity linking
- âŒ No relation extraction
- âŒ No knowledge graph
- âŒ No provenance tracking
- âŒ No coreference resolution
- âŒ No clickable citations with offsets
- âŒ No page mapping (PDFs)
- âŒ No "explain this answer" subgraphs

---

### Proposed State (Neo4j + Enhanced Pipeline)

```
URL â†’ Fetch â†’ Chunk â†’ Coref â†’ NER â†’ Link â†’ Relations â†’ Embed â†’ Index
         â†“      â†“       â†“      â†“      â†“       â†“         â†“        â†“
      MinIO  MinIO   MinIO  MinIO  MinIO   MinIO    MinIO   Neo4j + Qdrant
      raw/   derived/ coref  mentions links  relations embeddings  (dual index)
                â†“                                                    â†“
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Knowledge Graph
                                                                     â†“
                                                              â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
                                                              â†“             â†“
                                                         RAG with      Graph Viz
                                                         Citations     & Export
```

**New Capabilities**:
- âœ… Full provenance (doc â†’ chunk â†’ mention â†’ entity â†’ relation)
- âœ… Knowledge graph (entities and relationships)
- âœ… Clickable citations (byte offsets, page maps)
- âœ… Entity linking (canonical entity IDs)
- âœ… Coreference resolution (he/she/it â†’ actual entity)
- âœ… Relation extraction (EMPLOYED_BY, FOUNDED, CITES)
- âœ… Graph-guided retrieval (filter by entities, dates, relationships)
- âœ… "Explain this answer" subgraph exports
- âœ… Deterministic IDs (idempotent re-ingestion)

---

## ðŸ¤” Key Architectural Decisions

### Decision 1: Neo4j vs. Qdrant for Vectors?

**Option A: Neo4j Only (Recommended for MVP)**
- âœ… Simpler architecture (one database)
- âœ… Neo4j 5+ has native vector indexes
- âœ… Graph-guided retrieval in single query
- âœ… Lower operational complexity
- âœ… Good for single-user local setup
- âš ï¸ Vector performance less mature than Qdrant
- âš ï¸ May need Qdrant for scale later

**Option B: Hybrid Neo4j + Qdrant (Future)**
- âœ… Best of both: graph + specialized vector DB
- âœ… Graph prefilter â†’ Qdrant search (outlined in spec)
- âœ… Proven vector performance at scale
- âŒ More complexity (two DBs to manage)
- âŒ More expensive (cloud costs)
- âŒ Overkill for single-user

**Decision**: Start with **Neo4j-only** (Option A), add Qdrant as optional optimization in future sprint.

**Implementation**: Keep Qdrant code dormant, make vector store configurable.

---

### Decision 2: When to Implement This?

**Option A: Before FastAPI Sprint (Recommended)**
- âœ… FastAPI streams progress for NEW pipeline
- âœ… Don't build real-time streaming twice
- âœ… Foundation correct from start
- âŒ Delays visible progress (no UI updates yet)
- âŒ Larger scope before user-facing features

**Option B: After FastAPI Sprint**
- âœ… Get UI working faster
- âœ… Show incremental progress
- âŒ Have to rebuild FastAPI streaming for new pipeline
- âŒ Throw away current simple pipeline
- âŒ More total work

**Decision**: **Before FastAPI** (Option A). This is foundational.

---

### Decision 3: Migration Strategy?

**Option A: Big Bang (Replace Everything)**
- Rewrite entire pipeline at once
- High risk, long feedback cycle
- âŒ Not recommended

**Option B: Phased (3 Sprints)**
- Sprint 5: Neo4j + Basic NER/Linking
- Sprint 6: Relations + Provenance
- Sprint 7: FastAPI + Real-time Streaming
- âœ… Recommended approach

**Decision**: **Phased** (Option B) with clear milestones.

---

## ðŸ“‹ Adaptation to Williams Framework

### What to Keep from Spec

âœ… **Core Concepts**:
- Deterministic IDs (sha256-based `doc_id`)
- Provenance chains (doc â†’ chunk â†’ mention â†’ entity)
- MinIO structured layout (raw/ + derived/)
- Neo4j graph model (Document, Chunk, Entity, Relations)
- Clickable citations with offsets
- Idempotent re-ingestion
- Cost controls and privacy-first

âœ… **Pipeline Stages**:
1. Fetch/Normalize
2. Chunker (deterministic offsets)
3. Coreference (local model)
4. NER (spaCy)
5. Entity Linking (local alias table first)
6. Relation Extraction (precision-first)
7. Embeddings (per chunk + optional per entity)
8. Indexing (Neo4j with vector indexes)

âœ… **RAG Enhancements**:
- `/v1/rag/query` with citations
- `/v1/rag/explain` for subgraphs
- Graph export (GraphML, JSON)

---

### What to Adapt for Williams Framework

#### 1. Tier System Integration

**Spec**: No tier concept  
**Williams**: Keep tier-a/b/c/d quality scoring

**Adaptation**:
```python
# Add to Document node
:Document {
    doc_id: str,
    tier: str,           # NEW: tier-a through tier-d
    quality_score: float, # NEW: 0-10 screening score
    # ... existing fields
}

# MinIO layout becomes:
raw/{tier}/{doc_id}/source.ext
derived/{tier}/{doc_id}/manifest.json
# ... etc
```

**Benefit**: Maintain quality-first curation while adding provenance.

---

#### 2. Model Router Integration

**Spec**: Fixed model choices  
**Williams**: Dynamic model routing (nano/mini/standard)

**Adaptation**:
```python
class PipelineConfig:
    ner_model: ModelComplexity = ModelComplexity.MINI
    linking_model: ModelComplexity = ModelComplexity.STANDARD
    relation_model: ModelComplexity = ModelComplexity.STANDARD
    embedding_model: str = "bge-m3"
    
    # Cost tracking per stage
    cost_caps: dict[str, float] = {
        "ner": 0.10,      # per doc
        "linking": 0.50,  # per doc
        "relations": 1.00 # per doc
    }
```

**Benefit**: Preserve cost optimization strategy.

---

#### 3. Plugin Architecture

**Spec**: Hardcoded extractors  
**Williams**: Plugin registry for extensibility

**Adaptation**:
```python
# Keep existing plugin system
class NERPlugin(Plugin):
    async def extract_entities(
        self, 
        chunk: Chunk, 
        context: dict
    ) -> list[Mention]:
        """Plugin hook for custom NER models"""
        
class LinkingPlugin(Plugin):
    async def link_entity(
        self,
        mention: Mention,
        candidates: list[Entity],
        context: dict
    ) -> Link:
        """Plugin hook for custom entity linking"""
```

**Benefit**: Allow domain-specific NER/linking (legal, medical, technical).

---

#### 4. Existing Services Integration

**Spec**: New endpoints  
**Williams**: Extend existing services

**Adaptation**:
```python
# Enhance existing SearchService
class SearchService:
    async def semantic_search(
        self, 
        query: str,
        filters: SearchFilters  # NEW: entity, date, relation filters
    ) -> SearchResult:
        # Use Neo4j graph-guided retrieval
        
    async def explain_answer(
        self,
        chunk_ids: list[str]
    ) -> SubGraph:
        # NEW: Return provenance subgraph

# Enhance existing ContentService  
class ContentService:
    async def screen_url(self, url: str) -> ScreeningResult:
        # Keep existing screening logic
        
    async def process_content_advanced(
        self,
        source: ContentSource
    ) -> EnhancedPipelineResult:
        # NEW: Full NER/linking/relations pipeline
```

**Benefit**: Preserve existing functionality, add enhanced mode.

---

#### 5. Testing Patterns

**Spec**: Generic test approach  
**Williams**: TDD with real services (no mocks)

**Adaptation**:
```python
# tests/integration/pipeline/test_ner_real_neo4j.py

@pytest.fixture
async def neo4j_driver():
    """REAL Neo4j instance in Docker (no mocks)"""
    driver = GraphDatabase.driver(
        settings.neo4j_uri,
        auth=(settings.neo4j_user, settings.neo4j_password)
    )
    yield driver
    # Cleanup test data
    await cleanup_test_documents(driver)

@pytest.mark.asyncio
async def test_ner_pipeline_stores_entities_in_real_neo4j(neo4j_driver):
    """Should extract entities and store in REAL Neo4j"""
    # Ingest test document
    result = await pipeline.process_content_advanced(test_source)
    
    # Verify in REAL Neo4j
    entities = await neo4j_driver.execute_query(
        "MATCH (e:Entity)-[:MENTIONS]-(c:Chunk {chunk_id: $chunk_id}) RETURN e",
        chunk_id=result.chunks[0].chunk_id
    )
    
    assert len(entities) > 0
    assert entities[0]["type"] in ["PERSON", "ORG", "GPE"]
```

**Benefit**: Maintain 90%+ coverage with real integration tests.

---

## ðŸ—ºï¸ Phased Implementation Roadmap

### Sprint 5: Neo4j Foundation + Basic NER

**Duration**: 2 weeks  
**Goal**: Neo4j graph database with basic entity extraction

**Stories**:
1. **S5-501**: Neo4j Integration & Schema
   - Set up Neo4j container in docker-compose
   - Create node/relationship schema
   - Vector index creation
   - Connection pooling
   - **Tests**: 12 integration tests with real Neo4j

2. **S5-502**: Deterministic ID System
   - SHA256-based `doc_id`
   - Offset-based `chunk_id`
   - Mention/relation ID generators
   - **Tests**: 8 unit tests for ID determinism

3. **S5-503**: Enhanced Chunker with Provenance
   - Deterministic byte offsets
   - Heading path capture
   - Page map for PDFs (char â†’ page/bbox)
   - **Tests**: 10 tests (unit + integration with real PDFs)

4. **S5-504**: NER Pipeline (SpaCy)
   - Entity extraction (PERSON, ORG, GPE, etc.)
   - Confidence scoring
   - Model versioning
   - Store to Neo4j
   - **Tests**: 15 tests with real SpaCy model

**Deliverables**:
- âœ… Neo4j running in docker-compose
- âœ… Documents, Chunks, Entities in graph
- âœ… Basic NER working with confidence scores
- âœ… Provenance chain: doc â†’ chunk â†’ entity
- âœ… 45+ tests, 90%+ coverage

**Acceptance**: Ingest a test article â†’ see entities in Neo4j â†’ query by Cypher.

---

### Sprint 6: Entity Linking + Relations

**Duration**: 2 weeks  
**Goal**: Canonical entities and relationship extraction

**Stories**:
1. **S6-601**: Coreference Resolution
   - Integrate local coref model
   - Chain tracking per chunk
   - Store coref_chain_id in mentions
   - **Tests**: 10 tests with real coref examples

2. **S6-602**: Entity Linking System
   - Local alias table (CSV seed)
   - Candidate generation
   - Optional LLM reranking (budget-controlled)
   - Canonical entity IDs (eid)
   - **Tests**: 12 tests with real alias lookups

3. **S6-603**: Relation Extraction
   - Pattern-based extraction (EMPLOYED_BY, FOUNDED, CITES)
   - LLM verifier (precision-first)
   - Provenance storage (chunk_id, span, quote)
   - **Tests**: 15 tests with real text examples

4. **S6-604**: RAG with Citations
   - Enhance search to return citations
   - Include byte offsets and quotes
   - Page map integration for PDFs
   - **Tests**: 12 tests for citation accuracy

**Deliverables**:
- âœ… Coref chains in graph
- âœ… Canonical entities linked
- âœ… Relations extracted and stored
- âœ… RAG returns clickable citations
- âœ… 49+ tests, 90%+ coverage

**Acceptance**: Ask "Who founded OpenAI?" â†’ get answer with citation showing exact quote and page number.

---

### Sprint 7: FastAPI Real-time + Graph Viz

**Duration**: 2 weeks  
**Goal**: Real-time streaming UI + graph export

**Stories**:
1. **S7-701**: FastAPI Progress Streaming (adapted from previous plan)
   - Stream NEW pipeline stages: fetch, chunk, coref, ner, link, relations, embed, index
   - SSE events for each stage
   - **Tests**: 15 tests with real SSE

2. **S7-702**: Streamlit Ingestion UI
   - Submit URLs
   - Real-time progress display
   - Show entities extracted
   - **Tests**: 8 UI tests

3. **S7-703**: Graph Export APIs
   - `/v1/graph/export/json` (D3-ready)
   - `/v1/graph/export/graphml` (Cypher â†’ GraphML)
   - `/v1/rag/explain` (subgraph for answer)
   - **Tests**: 10 tests

4. **S7-704**: Graph Visualization Page
   - Streamlit page with D3.js or Plotly
   - Show entities and relations
   - Click nodes â†’ see provenance
   - **Tests**: 6 integration tests

**Deliverables**:
- âœ… FastAPI streaming for NEW pipeline
- âœ… Streamlit shows real-time NER progress
- âœ… Graph visualization working
- âœ… Export subgraphs as JSON/GraphML
- âœ… 39+ tests, 90%+ coverage

**Acceptance**: Ingest â†’ watch entities appear in real-time â†’ visualize knowledge graph â†’ export.

---

## ðŸ’° Cost & Effort Analysis

### Comparison

| Approach | Sprints | Tests | Lines of Code | Risk |
|----------|---------|-------|---------------|------|
| **Original Plan** (FastAPI only) | 1 | 40 | ~2,000 | Low |
| **Neo4j First** (Recommended) | 3 | 133+ | ~8,000 | Medium |
| **Neo4j After** (Not recommended) | 4 | 173+ | ~10,000 | High (rework) |

### Why Neo4j First is Less Work

**If we do FastAPI first, then Neo4j**:
1. Sprint 5: Build FastAPI streaming for OLD pipeline (40 tests, 2k LOC)
2. Sprint 6-7: Build Neo4j pipeline (93 tests, 6k LOC)
3. Sprint 8: **REBUILD** FastAPI to stream NEW pipeline (40 tests, 2k LOC - WASTED)

**Total**: 173 tests, ~10k LOC, **HIGH REWORK**

**If we do Neo4j first, then FastAPI**:
1. Sprint 5-6: Build Neo4j pipeline (94 tests, 6k LOC)
2. Sprint 7: Build FastAPI streaming for NEW pipeline (39 tests, 2k LOC)

**Total**: 133 tests, ~8k LOC, **NO REWORK**

**Savings**: 40 tests, ~2k LOC, **avoid complete FastAPI rebuild**

---

## ðŸŽ¯ Recommended Path Forward

### Immediate Actions (This Week)

1. **Review & Approve** this analysis
2. **Update business plan** to reflect knowledge graph capabilities
3. **Add Neo4j to docker-compose.yml**
4. **Create Sprint 5 detailed TDD plan** (Neo4j Foundation)
5. **Postpone** FastAPI streaming to Sprint 7

### Updated Sprint Sequence

```
âœ… Sprint 1-4: Foundation (COMPLETE)
â³ Sprint 5: Neo4j + NER (2 weeks) - START HERE
â³ Sprint 6: Linking + Relations (2 weeks)
â³ Sprint 7: FastAPI + Viz (2 weeks)
â³ Sprint 8: Polish + Demo (1 week)
```

**Total**: 7 weeks to complete advanced pipeline with real-time UI

---

## â“ Discussion Questions

### Technical
1. **Neo4j Edition**: Use Community (free) or Enterprise (paid)? Community sufficient for single-user.
2. **Vector Dimensions**: Stick with current embedding model or switch? (spec suggests bge-m3, 1024-dim)
3. **NER Model**: SpaCy transformer (accurate, slower) or statistical (fast, less accurate)?
4. **Coref Model**: Which local model? (AllenNLP, Hugging Face, SpanBERT?)

### Strategic
1. **Should we keep Qdrant dormant** or remove completely? Recommend: Keep config flag, dormant code.
2. **Entity linking web lookups**: Off by default per spec, but do we want Wikidata integration later? Recommend: Yes, post-MVP.
3. **Relation types**: Start with which set? Recommend: EMPLOYED_BY, FOUNDED, CITES, AUTHORED.

### Product
1. **Priority**: Graph viz vs. real-time progress? Both important, graph viz wins (more differentiation).
2. **Demo dataset**: What to showcase? Recommend: AI research papers (transformers, GPT, etc.) - aligns with business plan.

---

## ðŸ“Š Impact on Business Plan

### Enhanced Value Propositions

**Before** (Current):
> "Williams AI curates content, generates podcasts, and provides voice conversations with your knowledge base."

**After** (Neo4j + Provenance):
> "Williams AI builds a **provenance-tracked knowledge graph** from your research, enabling **AI answers with clickable citations**, **relationship discovery**, and **explainable reasoning paths**."

### New Competitive Advantages

| Competitor | Citations | Provenance | Knowledge Graph | Our Advantage |
|------------|-----------|------------|-----------------|---------------|
| ChatGPT | âŒ | âŒ | âŒ | **Full provenance + explainable answers** |
| Perplexity | âš¡ Basic | âŒ | âŒ | **Byte-level offsets + graph relationships** |
| Notion AI | âŒ | âŒ | âŒ | **Knowledge graph + entity linking** |
| **Williams AI** | âœ… | âœ… | âœ… | **Only integrated solution** |

### Investor Pitch Enhancement

**New slides**:
- "Provenance-First Architecture" - Every answer traceable to source
- "Knowledge Graph" - Entities and relationships, not just keywords
- "Explainable AI" - Show reasoning paths, not black boxes
- "Graph Visualization" - See how concepts connect

**Strengthens**:
- IP/defensibility (complex pipeline)
- Product differentiation (unique capabilities)
- Enterprise value (audit trails, compliance)
- Trust & transparency (verify every claim)

---

## âœ… Acceptance Criteria for This Decision

We proceed with Neo4j-first approach if:
- âœ… Team agrees this is foundational (not premature optimization)
- âœ… Effort estimate acceptable (3 sprints = 6 weeks)
- âœ… Business value clear (citations + graph > real-time streaming alone)
- âœ… Risk manageable (phased approach with tests)
- âœ… Docker setup straightforward (Neo4j container works)

---

## ðŸš€ Next Steps

**If Approved**:
1. Add Neo4j to `docker-compose.yml` (5.x with vector support)
2. Create `docs/SPRINT-5-NEO4J-TDD-PLAN.md` (detailed day-by-day)
3. Seed alias table CSV with test entities
4. Download SpaCy model: `python -m spacy download en_core_web_trf`
5. Start S5-501: Neo4j integration tests (RED phase)

**First Command**:
```bash
# Add Neo4j to docker-compose
# Then verify
sudo docker-compose up -d neo4j
sudo docker-compose logs neo4j
```

---

**Your call**: Should we proceed with Neo4j-first approach? Any concerns or modifications?
